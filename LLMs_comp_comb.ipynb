{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc8952f9-3bd0-444f-8889-a22f3b834361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:12:22.230413Z",
     "iopub.status.busy": "2025-04-22T14:12:22.229873Z",
     "iopub.status.idle": "2025-04-22T14:12:22.265827Z",
     "shell.execute_reply": "2025-04-22T14:12:22.265166Z",
     "shell.execute_reply.started": "2025-04-22T14:12:22.230392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install torch==2.2.2+cu121 torchvision==0.17.2+cu121 \\\n",
    "#             torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e33897d-d478-4fd2-8bfb-8a7c803828af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:19:41.606466Z",
     "iopub.status.busy": "2025-04-22T14:19:41.606009Z",
     "iopub.status.idle": "2025-04-22T14:19:41.623127Z",
     "shell.execute_reply": "2025-04-22T14:19:41.622454Z",
     "shell.execute_reply.started": "2025-04-22T14:19:41.606437Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip install transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0674996e-3fac-4c61-80c3-04221bcfb551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T20:00:15.776660Z",
     "iopub.status.busy": "2025-04-22T20:00:15.775987Z",
     "iopub.status.idle": "2025-04-22T20:00:20.192567Z",
     "shell.execute_reply": "2025-04-22T20:00:20.191751Z",
     "shell.execute_reply.started": "2025-04-22T20:00:15.776640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# %pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf8aae7-2427-4171-9ee7-0eb236993158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:11:44.953757Z",
     "iopub.status.busy": "2025-04-27T21:11:44.952988Z",
     "iopub.status.idle": "2025-04-27T21:11:59.309064Z",
     "shell.execute_reply": "2025-04-27T21:11:59.308132Z",
     "shell.execute_reply.started": "2025-04-27T21:11:44.953735Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:105: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import time\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc793408-e618-45e1-bbc6-57945d918686",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cb56062-1cac-40b6-8962-4d6ecd9b24b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:35:35.582239Z",
     "iopub.status.busy": "2025-04-27T21:35:35.581735Z",
     "iopub.status.idle": "2025-04-27T21:35:35.631729Z",
     "shell.execute_reply": "2025-04-27T21:35:35.631004Z",
     "shell.execute_reply.started": "2025-04-27T21:35:35.582209Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"TEST_prompts_v2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_samples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d61f41-2aca-4aa2-ae76-5fcfd0acedf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:36:00.423825Z",
     "iopub.status.busy": "2025-04-27T21:36:00.423354Z",
     "iopub.status.idle": "2025-04-27T21:36:00.460416Z",
     "shell.execute_reply": "2025-04-27T21:36:00.459698Z",
     "shell.execute_reply.started": "2025-04-27T21:36:00.423788Z"
    }
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный автоматический ассистент офтальмолога. Ты создаешь текстовые описания глазного дна по его параметрам.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a3d6c-b114-4b60-90dd-794a0e2051f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Saiga Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217da29-a6fb-40eb-84be-ab98b825b878",
   "metadata": {},
   "source": [
    "Первичная загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "876c08bf-56ad-4347-a563-6630935465b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T14:19:54.379073Z",
     "iopub.status.busy": "2025-04-22T14:19:54.378602Z",
     "iopub.status.idle": "2025-04-22T14:37:46.553022Z",
     "shell.execute_reply": "2025-04-22T14:37:46.552102Z",
     "shell.execute_reply.started": "2025-04-22T14:19:54.379052Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 14:19:56.152187: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-22 14:19:57.124428: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Fetching 4 files: 100%|██████████| 4/4 [08:52<00:00, 133.04s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [08:42<00:00, 130.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"IlyaGusev/saiga_llama3_8b\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model.eval()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2638e31-6858-4b3d-90bd-16aaedb324a7",
   "metadata": {},
   "source": [
    "Загрузка модели из локальной памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d919f9b5-1756-4fd7-81d9-a35dff8b5d38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T12:19:19.650757Z",
     "iopub.status.busy": "2025-04-27T12:19:19.650305Z",
     "iopub.status.idle": "2025-04-27T12:23:44.031466Z",
     "shell.execute_reply": "2025-04-27T12:23:44.030590Z",
     "shell.execute_reply.started": "2025-04-27T12:19:19.650737Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 12:19:21.760086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 12:19:22.705010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [04:20<00:00, 65.08s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/jupyter/datasphere/project/modelcache/models--IlyaGusev--saiga_llama3_8b/snapshots/5bb9917bdb85340549662ebb62c8e522037ff3f3'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1671b746-4799-44dc-8a2d-8633224a9c28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T12:23:55.484856Z",
     "iopub.status.busy": "2025-04-27T12:23:55.484375Z",
     "iopub.status.idle": "2025-04-27T12:23:55.508816Z",
     "shell.execute_reply": "2025-04-27T12:23:55.508125Z",
     "shell.execute_reply.started": "2025-04-27T12:23:55.484836Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"IlyaGusev/saiga_llama3_8b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87240a7-e0a0-436f-a520-5797bc175046",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T12:23:56.035765Z",
     "iopub.status.busy": "2025-04-27T12:23:56.034845Z",
     "iopub.status.idle": "2025-04-27T12:23:56.456593Z",
     "shell.execute_reply": "2025-04-27T12:23:56.455917Z",
     "shell.execute_reply.started": "2025-04-27T12:23:56.035739Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 128000,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 128009,\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"pad_token_id\": 128000,\n",
       "  \"repetition_penalty\": 1.12,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 30,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6750064-4150-4f87-b9c6-9cba8567c7df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T12:23:57.647937Z",
     "iopub.status.busy": "2025-04-27T12:23:57.647027Z",
     "iopub.status.idle": "2025-04-27T12:23:57.672049Z",
     "shell.execute_reply": "2025-04-27T12:23:57.671351Z",
     "shell.execute_reply.started": "2025-04-27T12:23:57.647915Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный автоматический ассистент офтальмолога. Ты создаешь текстовые описания глазного дна по его параметрам.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c7c68b5-7aea-4886-9c8c-d7fc997b1a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T12:24:02.145127Z",
     "iopub.status.busy": "2025-04-27T12:24:02.144654Z",
     "iopub.status.idle": "2025-04-27T12:36:35.927769Z",
     "shell.execute_reply": "2025-04-27T12:36:35.926824Z",
     "shell.execute_reply.started": "2025-04-27T12:24:02.145106Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_elements = []\n",
    "for sample in loaded_samples:\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": DEFAULT_SYSTEM_PROMPT\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": sample\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dataframe_elem = {\"text\":output, \"time\":total_time}\n",
    "    dataframe_elements.append(dataframe_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66fa94ca-4699-4461-8709-e3204a93d51a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T12:36:35.929412Z",
     "iopub.status.busy": "2025-04-27T12:36:35.929080Z",
     "iopub.status.idle": "2025-04-27T12:36:36.070313Z",
     "shell.execute_reply": "2025-04-27T12:36:36.069483Z",
     "shell.execute_reply.started": "2025-04-27T12:36:35.929390Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_elements)\n",
    "df.to_csv(\"Saiga_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae866727-3c76-4085-a1d5-8a76b181ab26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## YandexGPT-5 Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d2557a-1e02-4bc4-8fd6-1162cf4c026c",
   "metadata": {},
   "source": [
    "Первичная загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fce39c-2ba9-4c1b-917b-54b2640c0374",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"yandex/YandexGPT-5-Lite-8B-instruct\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model.eval()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3fa63-d90c-4d99-ad3b-add45ff52fa3",
   "metadata": {},
   "source": [
    "Загрузка модели из локальной памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a64278-5345-470d-915d-0d9a73f04bee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T11:36:46.830179Z",
     "iopub.status.busy": "2025-04-27T11:36:46.829707Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 11:36:53.845805: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 11:36:57.370592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "model_path = '/home/jupyter/datasphere/project/modelcache/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400c435-4a5d-46b1-ac1b-30d5037530a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"yandex/YandexGPT-5-Lite-8B-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676f986-291d-4b38-8740-a1ff5bda8322",
   "metadata": {},
   "source": [
    "Также установим ограничение на max_new_tokens = 1536, как и Saiga. Чтобы при этом обойти дефотное ограничение на 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "280fa085-5a22-407a-8ed4-8e3cef291274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T20:09:23.182716Z",
     "iopub.status.busy": "2025-04-22T20:09:23.182446Z",
     "iopub.status.idle": "2025-04-22T20:09:23.200623Z",
     "shell.execute_reply": "2025-04-22T20:09:23.200058Z",
     "shell.execute_reply.started": "2025-04-22T20:09:23.182697Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2efc2f-59e0-407f-ba09-e4815b08e67c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1536\n",
    "generation_config.pad_token_id = 2\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a494f89-b150-41f0-ad91-e5c52c8d887b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный автоматический ассистент офтальмолога. Ты создаешь текстовые описания глазного дна по его параметрам.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d66f56b-ce4f-41bf-8e77-d3a845e6b28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T20:09:33.357633Z",
     "iopub.status.busy": "2025-04-22T20:09:33.357230Z",
     "iopub.status.idle": "2025-04-22T20:15:44.427976Z",
     "shell.execute_reply": "2025-04-22T20:15:44.427318Z",
     "shell.execute_reply.started": "2025-04-22T20:09:33.357615Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_elements = []\n",
    "for sample in loaded_samples:\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": DEFAULT_SYSTEM_PROMPT\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": sample\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dataframe_elem = {\"text\":output, \"time\":total_time}\n",
    "    dataframe_elements.append(dataframe_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35cf339f-7194-4dfc-8cbd-7c4bef265dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T12:17:34.380072Z",
     "iopub.status.busy": "2025-04-27T12:17:34.379654Z",
     "iopub.status.idle": "2025-04-27T12:17:34.592829Z",
     "shell.execute_reply": "2025-04-27T12:17:34.591984Z",
     "shell.execute_reply.started": "2025-04-27T12:17:34.380052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_elements)\n",
    "df.to_csv(\"Yandex_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9edecce-4c71-4b36-afe8-172c75778746",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## BioMistral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147bc6b-05ee-4865-82e2-89464d5bdfe1",
   "metadata": {},
   "source": [
    "Первичная загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f859d8-9fc2-4d87-badf-aa746cb45daf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:04:38.968042Z",
     "iopub.status.busy": "2025-04-23T11:04:38.966681Z",
     "iopub.status.idle": "2025-04-23T11:21:49.627336Z",
     "shell.execute_reply": "2025-04-23T11:21:49.626118Z",
     "shell.execute_reply.started": "2025-04-23T11:04:38.968005Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 11:04:47.528423: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-23 11:04:53.645176: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"BioMistral/BioMistral-7B\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model.eval()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531096aa-1531-4b75-b67a-a4edb75ea5f3",
   "metadata": {},
   "source": [
    "Загрузка модели из локальной памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b4187e-bf48-4478-aec7-c7306945667c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = '/home/jupyter/datasphere/project/modelcache/models--BioMistral--BioMistral-7B'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1c1990d-de1b-4774-b891-40fbc9e2c155",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T12:52:11.208282Z",
     "iopub.status.busy": "2025-04-27T12:52:11.207856Z",
     "iopub.status.idle": "2025-04-27T12:52:11.220558Z",
     "shell.execute_reply": "2025-04-27T12:52:11.219779Z",
     "shell.execute_reply.started": "2025-04-27T12:52:11.208260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"BioMistral/BioMistral-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f127a62-758b-43df-b3d9-99e8e1068fb0",
   "metadata": {},
   "source": [
    "Также установим ограничение на max_new_tokens = 1536, как и Saiga. Чтобы при этом обойти дефотное ограничение на 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac841a3a-e103-42fa-af81-a56141ecfb89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:30:08.536108Z",
     "iopub.status.busy": "2025-04-23T11:30:08.535099Z",
     "iopub.status.idle": "2025-04-23T11:30:08.598001Z",
     "shell.execute_reply": "2025-04-23T11:30:08.596635Z",
     "shell.execute_reply.started": "2025-04-23T11:30:08.536073Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53c4c7a2-08c7-4a36-9fef-6ceddb5196ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:30:11.726136Z",
     "iopub.status.busy": "2025-04-23T11:30:11.724895Z",
     "iopub.status.idle": "2025-04-23T11:30:21.771899Z",
     "shell.execute_reply": "2025-04-23T11:30:21.770371Z",
     "shell.execute_reply.started": "2025-04-23T11:30:11.726089Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"pad_token_id\": 2\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1536\n",
    "generation_config.pad_token_id = 2\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7568007d-82b1-4b7f-b484-9bc19e978a10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:30:27.336045Z",
     "iopub.status.busy": "2025-04-23T11:30:27.334656Z",
     "iopub.status.idle": "2025-04-23T11:30:27.351976Z",
     "shell.execute_reply": "2025-04-23T11:30:27.350802Z",
     "shell.execute_reply.started": "2025-04-23T11:30:27.336008Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный автоматический ассистент офтальмолога. Ты создаешь текстовые описания глазного дна по его параметрам.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91297fcd-b6e2-4a72-9a89-fb6ee4fc7288",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:37:48.152540Z",
     "iopub.status.busy": "2025-04-23T11:37:48.151357Z",
     "iopub.status.idle": "2025-04-23T11:51:52.583571Z",
     "shell.execute_reply": "2025-04-23T11:51:52.582291Z",
     "shell.execute_reply.started": "2025-04-23T11:37:48.152486Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_elements = []\n",
    "for sample in loaded_samples:\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": sample\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dataframe_elem = {\"text\":output, \"time\":total_time}\n",
    "    dataframe_elements.append(dataframe_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b744d77-b31e-4ff8-8b48-64e69ab51221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T11:51:52.586888Z",
     "iopub.status.busy": "2025-04-23T11:51:52.586271Z",
     "iopub.status.idle": "2025-04-23T11:51:52.644489Z",
     "shell.execute_reply": "2025-04-23T11:51:52.643330Z",
     "shell.execute_reply.started": "2025-04-23T11:51:52.586849Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_elements)\n",
    "df.to_csv(\"BioMistal_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b663abb-eb2d-47d0-867f-ae95c04d10c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## TableLLama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5afae7-7437-4ab5-80c2-88cce072f4b9",
   "metadata": {},
   "source": [
    "Первичная загрузка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f28c40f-6a08-4fa6-aa5c-ee75f8f0ca19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T13:54:22.102203Z",
     "iopub.status.busy": "2025-04-23T13:54:22.101252Z",
     "iopub.status.idle": "2025-04-23T14:03:44.682129Z",
     "shell.execute_reply": "2025-04-23T14:03:44.680911Z",
     "shell.execute_reply.started": "2025-04-23T13:54:22.102172Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 13:54:29.908913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-23 13:54:35.761029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Fetching 2 files: 100%|██████████| 2/2 [04:53<00:00, 146.88s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:50<00:00, 115.45s/it]\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"osunlp/TableLlama\"\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     MODEL_NAME,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model.eval()\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9227f-82eb-44c0-bfdc-5204f3bd0b4d",
   "metadata": {},
   "source": [
    "Загрузка модели из локальной памяти"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50188d7f-c0e9-46cf-b1d7-05ffab2278af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T20:35:21.387380Z",
     "iopub.status.busy": "2025-04-27T20:35:21.386452Z",
     "iopub.status.idle": "2025-04-27T20:39:34.412848Z",
     "shell.execute_reply": "2025-04-27T20:39:34.411437Z",
     "shell.execute_reply.started": "2025-04-27T20:35:21.387330Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 20:35:28.411080: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 20:35:33.758901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:51<00:00, 115.57s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/jupyter/datasphere/project/modelcache/models--osunlp--TableLlama/snapshots/b4bd8bac8b7570dcfa01ca3ef4f8fd0ffef957ed'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75115ae5-a792-421c-a80f-ceaed97e9460",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T20:42:16.656836Z",
     "iopub.status.busy": "2025-04-27T20:42:16.655426Z",
     "iopub.status.idle": "2025-04-27T20:42:16.680595Z",
     "shell.execute_reply": "2025-04-27T20:42:16.679553Z",
     "shell.execute_reply.started": "2025-04-27T20:42:16.656794Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"osunlp/TableLlama\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3735c70-6bc8-4f2a-ba22-ab67976463cf",
   "metadata": {},
   "source": [
    "Также установим ограничение на max_new_tokens = 1536, как и Saiga. Чтобы при этом обойти дефотное ограничение на 20. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b92e6ca8-307e-4a24-aa7b-c01a182765c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T20:42:18.942018Z",
     "iopub.status.busy": "2025-04-27T20:42:18.940289Z",
     "iopub.status.idle": "2025-04-27T20:42:18.969846Z",
     "shell.execute_reply": "2025-04-27T20:42:18.968729Z",
     "shell.execute_reply.started": "2025-04-27T20:42:18.941967Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53471688-e05b-4146-af45-ab0979f867e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T20:42:19.418351Z",
     "iopub.status.busy": "2025-04-27T20:42:19.417240Z",
     "iopub.status.idle": "2025-04-27T20:42:19.768826Z",
     "shell.execute_reply": "2025-04-27T20:42:19.767731Z",
     "shell.execute_reply.started": "2025-04-27T20:42:19.418283Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"max_length\": 4096,\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"temperature\": 0.6,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1536\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8db8cfa-b4fb-4e2f-9005-06774386607b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T20:42:21.122979Z",
     "iopub.status.busy": "2025-04-27T20:42:21.121957Z",
     "iopub.status.idle": "2025-04-27T20:42:21.145261Z",
     "shell.execute_reply": "2025-04-27T20:42:21.144162Z",
     "shell.execute_reply.started": "2025-04-27T20:42:21.122938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный автоматический ассистент офтальмолога. Ты создаешь текстовые описания глазного дна по его параметрам.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "966473e8-840a-4c69-96f7-7aece8739845",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T20:42:24.571949Z",
     "iopub.status.busy": "2025-04-27T20:42:24.570594Z",
     "iopub.status.idle": "2025-04-27T20:42:24.589618Z",
     "shell.execute_reply": "2025-04-27T20:42:24.588533Z",
     "shell.execute_reply.started": "2025-04-27T20:42:24.571894Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"TEST_samples_v2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    json_samples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a65cc08-5eec-4275-8672-a518e595ebf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T20:44:05.670168Z",
     "iopub.status.busy": "2025-04-27T20:44:05.669031Z",
     "iopub.status.idle": "2025-04-27T21:03:50.727967Z",
     "shell.execute_reply": "2025-04-27T21:03:50.726482Z",
     "shell.execute_reply.started": "2025-04-27T20:44:05.670125Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_elements = []\n",
    "for sample_dict in json_samples:\n",
    "    # prompt = f\"{DEFAULT_SYSTEM_PROMPT}\\n {loaded_samples[i]}\\n\"\n",
    "    json_text = f\"\"\"\n",
    "    {{\n",
    "    \"ДЗН\": {{\n",
    "        \"Цвет\": \"{sample_dict[\"od_color\"]}\",\n",
    "        \"Монотонность\": \"{sample_dict[\"od_monotone\"]}\",\n",
    "        \"Размер\": \"{sample_dict[\"od_size\"]}\",\n",
    "        \"Форма\": \"{sample_dict[\"od_shape\"]}\",\n",
    "        \"Границы\": \"{sample_dict[\"od_border\"]}\",\n",
    "        \"Экскавация\": {{\n",
    "            \"Размер\": {sample_dict[\"od_excavation_size\"]},\n",
    "            \"Сектор\": {sample_dict[\"od_excavation_location\"]}\n",
    "        }},\n",
    "        \"Э/Д\": {sample_dict[\"od_excavation_ratio\"]},\n",
    "        \"Сосудистый пучок\": {sample_dict[\"od_vessels_location\"]},\n",
    "    }},\n",
    "    \"Сосуды\": {{\n",
    "        \"Артерии\": {{\n",
    "            \"Ход\": {sample_dict[\"vessels_art_course\"]},\n",
    "            \"Извитость\": {sample_dict[\"vessels_art_turtuosity\"]},\n",
    "            \"Бифуркация\": {sample_dict[\"vessels_art_bifurcation\"]},\n",
    "            \"Калибр\": {sample_dict[\"vessels_art_caliber\"]}\n",
    "        }},\n",
    "        \"Вены\": {{\n",
    "            \"Ход\": {sample_dict[\"vessels_vein_course\"]},\n",
    "            \"Извитость\": {sample_dict[\"vessels_vein_turtuosity\"]},\n",
    "            \"Бифуркация\": {sample_dict[\"vessels_vein_bifurcation\"]},\n",
    "            \"Калибр\": {sample_dict[\"vessels_vein_caliber\"]}\n",
    "        }},\n",
    "        \"А/В индекс\": {sample_dict[\"vessels_ratio\"]},\n",
    "    }},\n",
    "    \"Макула\": {{\n",
    "        \"Макулярный рефлекс\": {sample_dict[\"macula_macular_reflex\"]},\n",
    "        \"Фовеальный рефлекс\": {sample_dict[\"macula_foveal_reflex\"]},\n",
    "    }},\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"### Instruction:\n",
    "{DEFAULT_SYSTEM_PROMPT}\n",
    "\n",
    "### Input:\n",
    "{json_text}\n",
    "\n",
    "### Question:\n",
    "На основе приведённого JSON-документа, содержащего описание глазного дна, сгенерируй полное текстовое описание, придерживаясь стилистики и формата примера.\n",
    "\n",
    "**Пример 1:**\n",
    "ДЗН серый, монотонность не наблюдается, нормального размера, правильной формы, границы четкие. Экскавация нормального размера, расположена в центре, э/д=0.4. Сосудистый пучок расположен центрально.  \n",
    "Артерии и Вены имеют нормальный ход, нормальную извитость, бифуркация в норме и калибр нормальный, соотношение А/В = 2/3.\n",
    "Макулярный рефлекс отсутствует, фовеальный рефлекс нормальный.\n",
    "\n",
    "**Пример 2:**\n",
    "ДЗН серый, границы четкие, форма правильная, размер нормальный. Экскавация нормальная, в центре. Сосудистый пучок расположен центрально.  \n",
    "Артерии: ход смещён наружу, извитость нормальная, бифуркация под острым углом, калибр нормальный.\n",
    "Вены: ход нормальный, извитость нормальная, бифуркация нормальная, калибр суженный.\n",
    "Соотношение А/В = 2/3.\n",
    "Макулярный рефлекс отсутствует, фовеальный рефлекс нормальный.\n",
    "\n",
    "**Твоя задача:**  \n",
    "Создай полное текстовое описание глазного дна по JSON-документу, следуй телеграфному стилю и примеру выше.\n",
    "\n",
    "### Response:\"\"\"\n",
    "    \n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dataframe_elem = {\"text\":output, \"time\":total_time}\n",
    "    dataframe_elements.append(dataframe_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e33f4ab6-037f-413b-a1c2-1608bc061a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:03:50.731498Z",
     "iopub.status.busy": "2025-04-27T21:03:50.729942Z",
     "iopub.status.idle": "2025-04-27T21:03:50.805385Z",
     "shell.execute_reply": "2025-04-27T21:03:50.804279Z",
     "shell.execute_reply.started": "2025-04-27T21:03:50.731445Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_elements)\n",
    "df.to_csv(\"TableLlama_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5379d7d-986a-442b-b825-b1765d703318",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Saiga Gemma 3 12b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63d0fa6a-642e-4422-829d-2d7b9fa828bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:13:04.734818Z",
     "iopub.status.busy": "2025-04-27T21:13:04.734302Z",
     "iopub.status.idle": "2025-04-27T21:27:10.382052Z",
     "shell.execute_reply": "2025-04-27T21:27:10.381105Z",
     "shell.execute_reply.started": "2025-04-27T21:13:04.734797Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 21:13:11.297243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 21:13:14.868163: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Fetching 5 files: 100%|██████████| 5/5 [07:00<00:00, 84.09s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [06:35<00:00, 79.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForConditionalGeneration(\n",
       "  (vision_tower): SiglipVisionModel(\n",
       "    (vision_model): SiglipVisionTransformer(\n",
       "      (embeddings): SiglipVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "        (position_embedding): Embedding(4096, 1152)\n",
       "      )\n",
       "      (encoder): SiglipEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-26): 27 x SiglipEncoderLayer(\n",
       "            (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (self_attn): SiglipAttention(\n",
       "              (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "              (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SiglipMLP(\n",
       "              (activation_fn): PytorchGELUTanh()\n",
       "              (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
       "              (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "    (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  )\n",
       "  (language_model): Gemma3ForCausalLM(\n",
       "    (model): Gemma3TextModel(\n",
       "      (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 3840, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-47): 48 x Gemma3DecoderLayer(\n",
       "          (self_attn): Gemma3Attention(\n",
       "            (q_proj): Linear(in_features=3840, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
       "            (v_proj): Linear(in_features=3840, out_features=2048, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=3840, bias=False)\n",
       "            (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "            (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Gemma3MLP(\n",
       "            (gate_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
       "            (up_proj): Linear(in_features=3840, out_features=15360, bias=False)\n",
       "            (down_proj): Linear(in_features=15360, out_features=3840, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "          (post_attention_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "          (pre_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "          (post_feedforward_layernorm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Gemma3RMSNorm((3840,), eps=1e-06)\n",
       "      (rotary_emb): Gemma3RotaryEmbedding()\n",
       "      (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3840, out_features=262208, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"IlyaGusev/saiga_gemma3_12b\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f0fc98-0e19-4e3a-a408-06bf075c7b24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:30:22.382790Z",
     "iopub.status.busy": "2025-04-27T21:30:22.382358Z",
     "iopub.status.idle": "2025-04-27T21:30:27.938558Z",
     "shell.execute_reply": "2025-04-27T21:30:27.937842Z",
     "shell.execute_reply.started": "2025-04-27T21:30:22.382768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 2,\n",
      "  \"cache_implementation\": \"hybrid\",\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    1,\n",
      "    106\n",
      "  ],\n",
      "  \"max_new_tokens\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72b2bbe7-ddd2-4a03-81f9-e0b1ea29f30e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T21:39:33.367100Z",
     "iopub.status.busy": "2025-04-27T21:39:33.366654Z",
     "iopub.status.idle": "2025-04-27T22:08:11.033846Z",
     "shell.execute_reply": "2025-04-27T22:08:11.032847Z",
     "shell.execute_reply.started": "2025-04-27T21:39:33.367079Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_elements = []\n",
    "for sample in loaded_samples:\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": DEFAULT_SYSTEM_PROMPT\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": sample\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dataframe_elem = {\"text\":output, \"time\":total_time}\n",
    "    dataframe_elements.append(dataframe_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ecc87eb-1454-4c11-9e0d-b6bc6d0a4b80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-27T22:08:11.035490Z",
     "iopub.status.busy": "2025-04-27T22:08:11.035096Z",
     "iopub.status.idle": "2025-04-27T22:08:11.076399Z",
     "shell.execute_reply": "2025-04-27T22:08:11.075681Z",
     "shell.execute_reply.started": "2025-04-27T22:08:11.035465Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_elements)\n",
    "df.to_csv(\"Gemma3_12b_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240d320f",
   "metadata": {},
   "source": [
    "## RuAdapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc245b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 20:24:16.670457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 20:24:17.635629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [08:15<00:00, 123.85s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/jupyter/datasphere/project/RuadaptQwen2.5'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef7733",
   "metadata": {},
   "source": [
    "**NB** В конфиге генерации не прописано максиальное количество токенов на выходе, без этого их будет всего 20. \n",
    "\n",
    "Поэтому необходимо прописать это явно в параметрах генерации.\n",
    "\n",
    "Зададим значение 1536, как в конфиге у Сайги. Также изменим некоторые другие параметры, чтобы получать более строгие генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a44b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 145111,\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"pad_token_id\": 145109,\n",
       "  \"repetition_penalty\": 1.05,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 30,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"RefalMachine/RuadaptQwen2.5-7B-Lite-Beta\"\n",
    "\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1536\n",
    "generation_config.temperature = 0.2\n",
    "generation_config.top_k = 30\n",
    "generation_config.top_p = 0.9\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный автоматический ассистент офтальмолога. Ты создаешь текстовые описания глазного дна по его параметрам.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9f5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_elements = []\n",
    "for sample in loaded_samples:\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": DEFAULT_SYSTEM_PROMPT\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": sample\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dataframe_elem = {\"text\":output, \"time\":total_time}\n",
    "    dataframe_elements.append(dataframe_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_elements)\n",
    "df.to_csv(\"RuAdapt_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73e9d4",
   "metadata": {},
   "source": [
    "## T-lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1899cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 20:03:58.390163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-27 20:03:59.353016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151665, 3584, padding_idx=151643)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=151665, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"t-tech/T-lite-it-1.0\"\n",
    "PATH = '/home/jupyter/datasphere/project/modelcache/models--t-tech--T-lite-it-1.0/snapshots/fbabc76f32140416dc5b0ceef392c7778eec1312'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    PATH, \n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be0ea37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 151643,\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": [\n",
       "    151645,\n",
       "    151643\n",
       "  ],\n",
       "  \"max_new_tokens\": 1536,\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.05,\n",
       "  \"temperature\": 0.2,\n",
       "  \"top_k\": 30,\n",
       "  \"top_p\": 0.9\n",
       "}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "generation_config.max_new_tokens = 1536\n",
    "generation_config.temperature = 0.2\n",
    "generation_config.top_k = 30\n",
    "generation_config.top_p = 0.9\n",
    "generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edaec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный автоматический ассистент офтальмолога. Ты создаешь текстовые описания глазного дна по его параметрам.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b26fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_elements = []\n",
    "for sample in loaded_samples:\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": DEFAULT_SYSTEM_PROMPT\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": sample\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    output_ids = model.generate(**data, generation_config=generation_config)[0]\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dataframe_elem = {\"text\":output, \"time\":total_time}\n",
    "    dataframe_elements.append(dataframe_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_elements)\n",
    "df.to_csv(\"T_lite_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc3ef0",
   "metadata": {},
   "source": [
    "## Gemma 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13fa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8544207d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0d633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [05:09<00:00, 154.59s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [04:40<00:00, 140.15s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/gemma-3-4b-it\"\n",
    "token = '<hf–token>'\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\", token=token\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7fc0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_id, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_SYSTEM_PROMPT = \"Ты — русскоязычный автоматический ассистент офтальмолога. Ты создаешь текстовые описания глазного дна по его параметрам.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035c8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_elements = []\n",
    "for sample in loaded_samples:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": DEFAULT_SYSTEM_PROMPT}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": loaded_samples[0]}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    ).to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "    input_len = inputs[\"input_ids\"].shape[-1]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "        generation = generation[0][input_len:]\n",
    "\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    dataframe_elem = {\"text\":decoded, \"time\":total_time}\n",
    "    dataframe_elements.append(dataframe_elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataframe_elements)\n",
    "df.to_csv(\"Gemma3_4b_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789e306-21a7-42af-b822-c6e56c367b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
